import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.python.ops.numpy_ops import np_config
import os
import math
np_config.enable_numpy_behavior()

os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'


# Want to review the layers and also activation function

# Code to define model architecture
def diffusion_PINN():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(20, activation='tanh', input_shape=(1,))
    ])

    # Number of hidden layers
    for i in range(4):
        model.add(tf.keras.layers.Dense(64, activation='tanh'))

    # Output layer
    model.add(tf.keras.layers.Dense(1, activation='linear', name='output_layer'))

    return model


# Create an instance of model
model = diffusion_PINN()
normal_neural_net = diffusion_PINN()


# Loss function for physics informed loss
def pinns_loss():

    # Coefficients/constants
    D = 1.0  # Diffusion coefficient
    sigma_a = 0.5  # Absorption cross-section
    nu_Sigma_f = 0.2  # Fission-related term
    S = 2.0  # Source term - started with no net neutron source
    L = D / sigma_a  # L term (from book)

    x_phys = tf.constant(np.linspace(0, 10, 100), dtype=tf.float32) # Linearly space 100 points over domain

    # Compute the first and the second derivatives of phi_pred with respect to x
    with tf.GradientTape() as t1:
        t1.watch(x_phys)
        with tf.GradientTape() as t2:
            t2.watch(x_phys)
            phi_pred = model(x_phys)
        dphi_dx = t2.gradient(phi_pred, x_phys)
    d2phi_dx2 = t1.gradient(dphi_dx, x_phys)

    # Residual loss (from book)
    L_residual = (d2phi_dx2 - (1 / L ** 2) * phi_pred)  # rest of eq:  - nu_Sigma_f * phi_pred - S

    # Boundary condition losses

    # Boundary condition J(a) = 0
    L_residual += dphi_dx[-1]

    # Boundary condition phi(a) = 0
    L_residual += phi_pred[-1]

    L_residual += (1.4-phi_pred[0])# initial loss from phi(0) ~= 1.4

    # MSE of residual
    mse_residual = tf.reduce_mean(tf.square(L_residual))

    return mse_residual


# Traditional loss (MAE same as normal neural net)
def traditional_loss(y_true, y_pred):
    return tf.keras.losses.MeanSquaredError()(y_true, y_pred)

sum_trad_losses = []
sum_pinns_losses = []

# Combined loss of PINNS and NN
def combined_loss(y_true, y_pred):
    trad_loss = traditional_loss(y_true, y_pred)
    sum_trad_losses.append(trad_loss)
    pinn_loss_val = pinns_loss()
    sum_pinns_losses.append(pinn_loss_val)
    # Create a boolean tensor based on the condition
    condition = tf.less(y_true, 0.1)  # True for values less than 0.1, False otherwise

    total_loss = trad_loss + 2*pinn_loss_val
    # Apply different weights based on the condition
    #total_loss = tf.where(condition, 100 * (trad_loss + pinn_loss_val), trad_loss + pinn_loss_val)
    return total_loss

# Compile model
model.compile(loss=combined_loss, optimizer='adam', run_eagerly=True)
normal_neural_net.compile(loss=['MeanAbsoluteError'], optimizer='adam')

# Define parameter ranges
param_ranges = {
    'D': (0.5, 2.5),       # Range for Diffusion coefficient
    'sigma_a': (0.3, 0.7), # Range for Absorption cross-section
    'nu_Sigma_f': (0.1, 0.3), # Range for Fission-related term
    'S': (1.5, 2.5),       # Range for Source term
    'a': (8.0, 12.0)       # Range for Length of the domain
}

val_D = 2.0
val_sigma_a = 0.5
val_nu_sigma_f = 0.2
val_S = 2.0
val_a = 10.0
val_L = math.sqrt(val_D / val_sigma_a)

# x is number of points for data and spaced over a certain part
num_points = 10
x = np.linspace(0, 10 , num_points)
x_val_points = np.linspace(0, 10, 100)


# Generate training data (use random values from ranges)
def generate_training_data(param_ranges, num_samples):
    param_samples = {}
    for param, (min_val, max_val) in param_ranges.items():
        param_samples[param] = np.random.uniform(min_val, max_val, num_samples).astype(np.float32)
    x_train = np.linspace(0, 10, num_samples).reshape(-1, 1)
    D = param_samples['D']
    sigma_a = param_samples['sigma_a']
    nu_Sigma_f = param_samples['nu_Sigma_f']
    S = param_samples['S']
    a = param_samples['a']
    L = np.sqrt(D / sigma_a)
    phi_train_true = ((L * S) / (2 * D)) * np.exp(-(np.abs(x_train) / L))
    return x_train, phi_train_true

def generate_validation_data(x):
    phi_true = ((val_L*val_S)/(2*val_D)) * np.exp(-(abs(x)/ val_L))
    return x, phi_true


# Train the model
num_samples = 10000  # Number of samples for training
x_train, phi_train_true = generate_training_data(param_ranges, num_samples)
x_test, phi_test = generate_validation_data(x_val_points)

num_epochs = 50  # Change if necessary
batch_size = 128


# Generate data for both models
history = model.fit(x_train, phi_train_true, epochs=num_epochs, batch_size=batch_size)
history_nn = normal_neural_net.fit(x_train, phi_train_true, epochs=num_epochs, batch_size=batch_size)


# Plot analytical solution, traditional NN and PINN
plt.plot(x_test, phi_test, label='Analytical solution')
plt.plot(x_test, model.predict(x_test), label='PINN')
plt.plot(x_test, normal_neural_net.predict(x_test), label='Traditional NN')
# plt.scatter(x_train, phi_train_true, label = 'Data points')
plt.legend()
plt.show()

plt.plot(sum_pinns_losses, label='Pinns losses')
plt.plot(sum_trad_losses, label='NN losses')
plt.xlabel('Epochs')
plt.ylim([0, 0.02])
plt.legend()
plt.show()


predicted_pinn = model.predict(x_test)
predicted_nn = normal_neural_net.predict(x_test)


# Function to display r^2, RMSE, MAE, max error and max % error for model
def model_metrics(values, name):
    predicted_data = np.array(values, dtype=np.float32)
    y_true = np.array(phi_test, dtype=np.float32)
    from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, max_error, r2_score
    print(f'r^2 ({name}) = {r2_score(y_true, predicted_data)}')
    print(f'RMSE ({name}) = {np.sqrt(mean_squared_error(y_true, predicted_data))}')
    print(f'MAE ({name}) = {mean_absolute_error(y_true, predicted_data)}')
    print(f'MAE PERCENTAGE ({name}) = {mean_absolute_percentage_error(y_true, predicted_data) * 100000}')
    print(f'MAX ERROR ({name}) = {max_error(y_true, predicted_data) * 100000}')


model_metrics(predicted_pinn, 'PINN')
model_metrics(predicted_nn, 'NN')
