import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.python.ops.numpy_ops import np_config
import os
import math
np_config.enable_numpy_behavior()

os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'


# Want to review the layers and also activation function

# Code to define model architecture
def diffusion_PINN():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(20, activation='tanh', input_shape=(1,))
    ])

    # Number of hidden layers
    for i in range(4):
        model.add(tf.keras.layers.Dense(64, activation='tanh'))

    # Output layer
    model.add(tf.keras.layers.Dense(1, activation='linear', name='output_layer'))

    return model


# Create an instance of model
model = diffusion_PINN()
normal_neural_net = diffusion_PINN()


# Loss function for physics informed loss
def pinns_loss():

    # Coefficients/constants
    D = 1.0  # Diffusion coefficient
    sigma_a = 0.5  # Absorption cross-section
    nu_Sigma_f = 0.2  # Fission-related term
    S = 2.0  # Source term - started with no net neutron source
    L = D / sigma_a  # L term (from book)

    x_phys = tf.constant(np.linspace(0, a, 100), dtype=tf.float32) # Linearly space 100 points over domain

    # Compute the first and the second derivatives of phi_pred with respect to x
    with tf.GradientTape() as t1:
        t1.watch(x_phys)
        with tf.GradientTape() as t2:
            t2.watch(x_phys)
            phi_pred = model(x_phys)
        dphi_dx = t2.gradient(phi_pred, x_phys)
    d2phi_dx2 = t1.gradient(dphi_dx, x_phys)

    # Residual loss (from book)
    L_residual = (d2phi_dx2 - (1 / L ** 2) * phi_pred)  # rest of eq:  - nu_Sigma_f * phi_pred - S

    # Boundary condition losses

    # Boundary condition J(a) = 0
    L_residual += dphi_dx[-1]

    # Boundary condition phi(a) = 0
    L_residual += phi_pred[-1]

    L_residual += (1.4-phi_pred[0])# initial loss from phi(0) ~= 1.4

    # MSE of residual
    mse_residual = tf.reduce_mean(tf.square(L_residual))

    return mse_residual


# Traditional loss (MAE same as normal neural net)
def traditional_loss(y_true, y_pred):
    return tf.keras.losses.MeanSquaredError()(y_true, y_pred)


decay_rate=0.2
# Define parameters for the problem
D = 1.0  # Diffusion coefficient
sigma_a = 0.5  # Absorption cross-section
nu_Sigma_f = 0.2  # Fission-related term
S = 2.0  # Source term - set as 2 for now
a = 10.0  # Length of the domain
L = math.sqrt(D / sigma_a)  # L^2 = D/sigma_a


# Inverted bell curve shape (parabola (0.5x-2.5)^2 + 1)

def inverted_bell_curve_weights(domain):
    weights = ((0.5*domain-2.5)**2)+1
    return weights

num_points = 4
x = np.linspace(2, 8, num_points)
x_val_points = np.linspace(0, a, 100)


# Combined loss of PINNS and NN - wrapping function to calc weights and custom loss function inside
def combined_loss_with_positional_weight(x_weight):
    weights = inverted_bell_curve_weights(x_weight)
    def custom_loss(y_true, y_pred):
        trad_loss = traditional_loss(y_true, y_pred)
        pinn_loss_val = pinns_loss()
        total_loss = tf.reduce_mean(weights * (trad_loss + pinn_loss_val))  # Apply weights to each data point
        return total_loss
    return custom_loss


# Compile model
model.compile(loss=combined_loss_with_positional_weight(x_val_points), optimizer='adam', run_eagerly=True)
normal_neural_net.compile(loss=['MeanAbsoluteError'], optimizer='adam')


# Generate training data
def generate_training_data(x):
    # Calculate the true neutron flux based on the diffusion equation pg 51 in Stacey
    phi_true = ((L * S) / (2 * D)) * np.exp(-(abs(x) / L))
    return x, phi_true


# Train the model
x_train, phi_train_true = generate_training_data(x)
x_test, phi_test = generate_training_data(x_val_points)


num_epochs = 2000 # Change if necessary
batch_size = 32


# Generate data for both models
history = model.fit(x_train, phi_train_true, epochs=num_epochs, batch_size=batch_size)
history_nn = normal_neural_net.fit(x_train, phi_train_true, epochs=num_epochs, batch_size=batch_size)


# Plot analytical solution, traditional NN and PINN
plt.plot(x_test, phi_test, label='Analytical solution')
plt.plot(x_test, model.predict(x_test), label='PINN')
plt.plot(x_test, normal_neural_net.predict(x_test), label='Traditional NN')
plt.scatter(x_train, phi_train_true, label = 'Data points')
plt.legend()
plt.show()


predicted_pinn = model.predict(x_test)
predicted_nn = normal_neural_net.predict(x_test)


# Function to display r^2, RMSE, MAE, max error and max % error for model
def model_metrics(values, name):
    predicted_data = np.array(values, dtype=np.float32)
    y_true = np.array(phi_test, dtype=np.float32)
    from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, max_error, r2_score
    print(f'r^2 ({name}) = {r2_score(y_true, predicted_data)}')
    print(f'RMSE ({name}) = {np.sqrt(mean_squared_error(y_true, predicted_data))}')
    print(f'MAE ({name}) = {mean_absolute_error(y_true, predicted_data)}')
    print(f'MAE PERCENTAGE ({name}) = {mean_absolute_percentage_error(y_true, predicted_data) * 100000}')
    print(f'MAX ERROR ({name}) = {max_error(y_true, predicted_data) * 100000}')


model_metrics(predicted_pinn, 'PINN')
model_metrics(predicted_nn, 'NN')
